"""
Shared Metrics Module for Project Athena.

Provides Prometheus-compatible metrics for tool execution, pipeline latency,
and other observability data.

Usage:
    from shared.metrics import (
        TOOL_EXECUTION_COUNT,
        TOOL_EXECUTION_LATENCY,
        record_tool_execution,
    )

    # Record a tool execution
    record_tool_execution(
        tool_name="get_weather",
        source="static",
        success=True,
        latency_seconds=0.5,
        guest_mode=False,
    )
"""
import time
from typing import Optional
from contextlib import contextmanager
import structlog

try:
    from prometheus_client import Counter, Histogram, Gauge, REGISTRY, CollectorRegistry
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False

logger = structlog.get_logger()

# =============================================================================
# Prometheus Metrics (if available)
# =============================================================================

if PROMETHEUS_AVAILABLE:
    # Tool Execution Metrics
    TOOL_EXECUTION_COUNT = Counter(
        'athena_tool_execution_total',
        'Total number of tool executions',
        ['tool_name', 'source', 'success', 'guest_mode']
    )

    TOOL_EXECUTION_LATENCY = Histogram(
        'athena_tool_execution_seconds',
        'Tool execution latency in seconds',
        ['tool_name', 'source'],
        buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0]
    )

    TOOL_EXECUTION_ERRORS = Counter(
        'athena_tool_execution_errors_total',
        'Total number of tool execution errors',
        ['tool_name', 'source', 'error_type']
    )

    # Tool Registry Metrics
    TOOL_REGISTRY_SIZE = Gauge(
        'athena_tool_registry_size',
        'Number of tools in registry by source',
        ['source']
    )

    TOOL_REGISTRY_REFRESH_COUNT = Counter(
        'athena_tool_registry_refresh_total',
        'Number of tool registry refreshes',
        ['success']
    )

    TOOL_REGISTRY_REFRESH_LATENCY = Histogram(
        'athena_tool_registry_refresh_seconds',
        'Tool registry refresh latency in seconds',
        buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
    )

    # MCP Security Metrics
    MCP_DOMAIN_BLOCKED = Counter(
        'athena_mcp_domain_blocked_total',
        'Number of blocked MCP domain requests',
        ['domain']
    )

    MCP_TOOL_BLOCKED = Counter(
        'athena_mcp_tool_blocked_total',
        'Number of blocked MCP tools',
        ['tool_name', 'reason']
    )

    # =============================================================================
    # Pipeline Timing Metrics (granular execution tracking)
    # =============================================================================

    # Pre-graph stage timing
    PRE_GRAPH_DURATION = Histogram(
        'athena_pre_graph_duration_seconds',
        'Pre-graph stage execution duration in seconds',
        ['stage'],
        buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
    )

    # Graph node timing (with optional substage)
    GRAPH_NODE_DURATION = Histogram(
        'athena_graph_node_duration_seconds',
        'Graph node execution duration in seconds',
        ['node', 'substage'],
        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    )

    # Post-graph stage timing
    POST_GRAPH_DURATION = Histogram(
        'athena_post_graph_duration_seconds',
        'Post-graph stage execution duration in seconds',
        ['stage'],
        buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5]
    )

    # LLM call timing (detailed)
    LLM_CALL_DURATION = Histogram(
        'athena_llm_call_duration_seconds',
        'LLM inference call duration in seconds',
        ['stage', 'model', 'call_type'],
        buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0]
    )

    # LLM tokens generated
    LLM_TOKENS_GENERATED = Counter(
        'athena_llm_tokens_generated_total',
        'Total tokens generated by LLM calls',
        ['stage', 'model']
    )

    # Total request timing
    REQUEST_TOTAL_DURATION = Histogram(
        'athena_request_total_duration_seconds',
        'Total request duration in seconds',
        buckets=[0.5, 1.0, 2.0, 3.0, 5.0, 7.5, 10.0, 15.0, 30.0]
    )

else:
    # Fallback stubs when prometheus_client is not available
    class StubMetric:
        """Stub metric that does nothing."""
        def labels(self, *args, **kwargs):
            return self
        def inc(self, *args, **kwargs):
            pass
        def observe(self, *args, **kwargs):
            pass
        def set(self, *args, **kwargs):
            pass

    TOOL_EXECUTION_COUNT = StubMetric()
    TOOL_EXECUTION_LATENCY = StubMetric()
    TOOL_EXECUTION_ERRORS = StubMetric()
    TOOL_REGISTRY_SIZE = StubMetric()
    TOOL_REGISTRY_REFRESH_COUNT = StubMetric()
    TOOL_REGISTRY_REFRESH_LATENCY = StubMetric()
    MCP_DOMAIN_BLOCKED = StubMetric()
    MCP_TOOL_BLOCKED = StubMetric()
    # Timing metrics stubs
    PRE_GRAPH_DURATION = StubMetric()
    GRAPH_NODE_DURATION = StubMetric()
    POST_GRAPH_DURATION = StubMetric()
    LLM_CALL_DURATION = StubMetric()
    LLM_TOKENS_GENERATED = StubMetric()
    REQUEST_TOTAL_DURATION = StubMetric()


# =============================================================================
# Helper Functions
# =============================================================================

def record_tool_execution(
    tool_name: str,
    source: str,
    success: bool,
    latency_seconds: float,
    guest_mode: bool = False,
    error_type: Optional[str] = None,
):
    """
    Record a tool execution metric.

    Args:
        tool_name: Name of the executed tool
        source: Tool source (static, mcp, legacy)
        success: Whether execution succeeded
        latency_seconds: Execution time in seconds
        guest_mode: Whether executed in guest mode
        error_type: Type of error if failed (e.g., 'timeout', 'api_error')
    """
    success_str = "true" if success else "false"
    guest_mode_str = "true" if guest_mode else "false"

    TOOL_EXECUTION_COUNT.labels(
        tool_name=tool_name,
        source=source,
        success=success_str,
        guest_mode=guest_mode_str,
    ).inc()

    TOOL_EXECUTION_LATENCY.labels(
        tool_name=tool_name,
        source=source,
    ).observe(latency_seconds)

    if not success and error_type:
        TOOL_EXECUTION_ERRORS.labels(
            tool_name=tool_name,
            source=source,
            error_type=error_type,
        ).inc()

    # Also log for structlog-based observability
    logger.info(
        "tool_execution",
        tool_name=tool_name,
        source=source,
        success=success,
        latency_seconds=round(latency_seconds, 3),
        guest_mode=guest_mode,
        error_type=error_type,
    )


def update_registry_size(static_count: int, mcp_count: int, legacy_count: int):
    """
    Update tool registry size metrics.

    Args:
        static_count: Number of static tools
        mcp_count: Number of MCP tools
        legacy_count: Number of legacy tools
    """
    TOOL_REGISTRY_SIZE.labels(source="static").set(static_count)
    TOOL_REGISTRY_SIZE.labels(source="mcp").set(mcp_count)
    TOOL_REGISTRY_SIZE.labels(source="legacy").set(legacy_count)


def record_registry_refresh(success: bool, latency_seconds: float):
    """
    Record a tool registry refresh.

    Args:
        success: Whether refresh succeeded
        latency_seconds: Refresh time in seconds
    """
    success_str = "true" if success else "false"
    TOOL_REGISTRY_REFRESH_COUNT.labels(success=success_str).inc()
    TOOL_REGISTRY_REFRESH_LATENCY.observe(latency_seconds)


def record_mcp_domain_blocked(domain: str):
    """Record a blocked MCP domain."""
    MCP_DOMAIN_BLOCKED.labels(domain=domain).inc()


def record_mcp_tool_blocked(tool_name: str, reason: str):
    """Record a blocked MCP tool."""
    MCP_TOOL_BLOCKED.labels(tool_name=tool_name, reason=reason).inc()


def record_timing_metrics(timing_data: dict):
    """
    Record pipeline timing metrics to Prometheus.

    Args:
        timing_data: Dict from TimingTracker.finalize() containing:
            - total_ms: Total request time
            - pre_graph: Dict of pre-graph stage timings
            - graph: Dict of graph node timings (may have nested substages)
            - post_graph: Dict of post-graph stage timings
            - llm_calls: List of LLM call details
    """
    # Record total request duration
    total_seconds = timing_data.get("total_ms", 0) / 1000.0
    REQUEST_TOTAL_DURATION.observe(total_seconds)

    # Record pre-graph stage timings
    pre_graph = timing_data.get("pre_graph", {})
    for key, value in pre_graph.items():
        if key == "total_ms":
            continue
        if key.endswith("_ms") and isinstance(value, (int, float)):
            stage_name = key[:-3]  # Remove _ms suffix
            PRE_GRAPH_DURATION.labels(stage=stage_name).observe(value / 1000.0)

    # Record graph node timings
    graph = timing_data.get("graph", {})
    for key, value in graph.items():
        if key == "total_ms":
            continue
        if isinstance(value, dict):
            # Nested node with substages
            node_name = key
            for subkey, subvalue in value.items():
                if subkey == "total_ms":
                    continue
                if subkey.endswith("_ms") and isinstance(subvalue, (int, float)):
                    substage_name = subkey[:-3]
                    GRAPH_NODE_DURATION.labels(
                        node=node_name,
                        substage=substage_name
                    ).observe(subvalue / 1000.0)
        elif key.endswith("_ms") and isinstance(value, (int, float)):
            # Flat node timing
            node_name = key[:-3]
            GRAPH_NODE_DURATION.labels(
                node=node_name,
                substage="total"
            ).observe(value / 1000.0)

    # Record post-graph stage timings
    post_graph = timing_data.get("post_graph", {})
    for key, value in post_graph.items():
        if key == "total_ms":
            continue
        if key.endswith("_ms") and isinstance(value, (int, float)):
            stage_name = key[:-3]
            POST_GRAPH_DURATION.labels(stage=stage_name).observe(value / 1000.0)

    # Record LLM call timings
    llm_calls = timing_data.get("llm_calls", [])
    for call in llm_calls:
        stage = call.get("stage", "unknown")
        model = call.get("model", "unknown")
        call_type = call.get("call_type", "inference")
        duration_ms = call.get("duration_ms", 0)
        tokens = call.get("tokens", 0)

        LLM_CALL_DURATION.labels(
            stage=stage,
            model=model,
            call_type=call_type
        ).observe(duration_ms / 1000.0)

        LLM_TOKENS_GENERATED.labels(
            stage=stage,
            model=model
        ).inc(tokens)


@contextmanager
def time_tool_execution(tool_name: str, source: str):
    """
    Context manager for timing tool execution.

    Usage:
        with time_tool_execution("get_weather", "static") as timer:
            result = await execute_tool()
            timer.set_success(True)

    Args:
        tool_name: Name of the tool
        source: Tool source
    """
    class ExecutionTimer:
        def __init__(self):
            self.success = False
            self.guest_mode = False
            self.error_type = None

        def set_success(self, success: bool):
            self.success = success

        def set_guest_mode(self, guest_mode: bool):
            self.guest_mode = guest_mode

        def set_error_type(self, error_type: str):
            self.error_type = error_type

    timer = ExecutionTimer()
    start_time = time.time()

    try:
        yield timer
    finally:
        latency = time.time() - start_time
        record_tool_execution(
            tool_name=tool_name,
            source=source,
            success=timer.success,
            latency_seconds=latency,
            guest_mode=timer.guest_mode,
            error_type=timer.error_type,
        )


# =============================================================================
# Metrics Endpoint Helper
# =============================================================================

def get_metrics_text() -> str:
    """
    Get Prometheus metrics in text format.

    Returns:
        Prometheus metrics as text for /metrics endpoint
    """
    if PROMETHEUS_AVAILABLE:
        from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
        return generate_latest().decode('utf-8')
    else:
        return "# prometheus_client not available\n"


# =============================================================================
# Service-Level Metrics (for RAG services, Gateway, etc.)
# =============================================================================

if PROMETHEUS_AVAILABLE:
    # HTTP request metrics
    HTTP_REQUEST_DURATION = Histogram(
        'athena_http_request_duration_seconds',
        'HTTP request duration in seconds',
        ['service', 'method', 'endpoint', 'status'],
        buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    )

    HTTP_REQUESTS_TOTAL = Counter(
        'athena_http_requests_total',
        'Total HTTP requests',
        ['service', 'method', 'endpoint', 'status']
    )

    HTTP_REQUESTS_IN_PROGRESS = Gauge(
        'athena_http_requests_in_progress',
        'HTTP requests currently in progress',
        ['service', 'method']
    )

    # External API call metrics
    EXTERNAL_API_DURATION = Histogram(
        'athena_external_api_duration_seconds',
        'External API call duration in seconds',
        ['service', 'api_name', 'status'],
        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    )

    EXTERNAL_API_CALLS_TOTAL = Counter(
        'athena_external_api_calls_total',
        'Total external API calls',
        ['service', 'api_name', 'status']
    )

    # Cache metrics
    CACHE_OPERATIONS_TOTAL = Counter(
        'athena_cache_operations_total',
        'Total cache operations',
        ['service', 'operation', 'result']
    )

    # Service info
    SERVICE_INFO = Gauge(
        'athena_service_info',
        'Service information (value is always 1)',
        ['service', 'version', 'port']
    )

else:
    HTTP_REQUEST_DURATION = StubMetric()
    HTTP_REQUESTS_TOTAL = StubMetric()
    HTTP_REQUESTS_IN_PROGRESS = StubMetric()
    EXTERNAL_API_DURATION = StubMetric()
    EXTERNAL_API_CALLS_TOTAL = StubMetric()
    CACHE_OPERATIONS_TOTAL = StubMetric()
    SERVICE_INFO = StubMetric()


# =============================================================================
# Service Metrics Helper Functions
# =============================================================================

def record_http_request(
    service: str,
    method: str,
    endpoint: str,
    status: str,
    duration_seconds: float
):
    """
    Record an HTTP request metric.

    Args:
        service: Service name (e.g., "weather-rag", "gateway")
        method: HTTP method (GET, POST, etc.)
        endpoint: Request endpoint/path
        status: Response status category (2xx, 4xx, 5xx)
        duration_seconds: Request duration in seconds
    """
    HTTP_REQUEST_DURATION.labels(
        service=service,
        method=method,
        endpoint=endpoint,
        status=status
    ).observe(duration_seconds)

    HTTP_REQUESTS_TOTAL.labels(
        service=service,
        method=method,
        endpoint=endpoint,
        status=status
    ).inc()

    logger.debug(
        "http_request",
        service=service,
        method=method,
        endpoint=endpoint,
        status=status,
        duration_ms=round(duration_seconds * 1000, 2)
    )


def record_external_api_call(
    service: str,
    api_name: str,
    status: str,
    duration_seconds: float
):
    """
    Record an external API call metric.

    Args:
        service: Service making the call
        api_name: External API name (e.g., "openweather", "google_places")
        status: Call status ("success", "error", "timeout")
        duration_seconds: Call duration in seconds
    """
    EXTERNAL_API_DURATION.labels(
        service=service,
        api_name=api_name,
        status=status
    ).observe(duration_seconds)

    EXTERNAL_API_CALLS_TOTAL.labels(
        service=service,
        api_name=api_name,
        status=status
    ).inc()

    logger.debug(
        "external_api_call",
        service=service,
        api_name=api_name,
        status=status,
        duration_ms=round(duration_seconds * 1000, 2)
    )


def record_cache_operation(service: str, operation: str, hit: bool):
    """
    Record a cache operation.

    Args:
        service: Service name
        operation: Cache operation (get, set, delete)
        hit: Whether it was a cache hit
    """
    result = "hit" if hit else "miss"
    CACHE_OPERATIONS_TOTAL.labels(
        service=service,
        operation=operation,
        result=result
    ).inc()


def register_service(service: str, version: str = "1.0.0", port: int = 0):
    """
    Register a service in metrics.

    Args:
        service: Service name
        version: Service version
        port: Service port
    """
    SERVICE_INFO.labels(
        service=service,
        version=version,
        port=str(port)
    ).set(1)

    logger.info(
        "service_registered_metrics",
        service=service,
        version=version,
        port=port
    )


@contextmanager
def time_external_api(service: str, api_name: str):
    """
    Context manager for timing external API calls.

    Usage:
        with time_external_api("weather-rag", "openweather") as timer:
            response = await http_client.get(url)
            timer.set_success(response.status_code == 200)

    Args:
        service: Service name
        api_name: External API name
    """
    class APITimer:
        def __init__(self):
            self.status = "success"

        def set_success(self, success: bool):
            self.status = "success" if success else "error"

        def set_status(self, status: str):
            self.status = status

    timer = APITimer()
    start_time = time.time()

    try:
        yield timer
    except Exception as e:
        timer.status = "error"
        raise
    finally:
        duration = time.time() - start_time
        record_external_api_call(service, api_name, timer.status, duration)


import functools

def timed_external_api(service: str, api_name: str):
    """
    Decorator for timing async external API call functions.

    Usage:
        @timed_external_api("weather-rag", "openweather")
        async def fetch_weather(lat, lon):
            response = await http_client.get(url)
            return response.json()

    Args:
        service: Service name
        api_name: External API name

    Returns:
        Decorated async function with timing
    """
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            status = "success"
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                record_external_api_call(service, api_name, status, duration)
        return wrapper
    return decorator


# =============================================================================
# FastAPI Integration
# =============================================================================

def setup_metrics_endpoint(app, service_name: str, port: int = 0, version: str = "1.0.0"):
    """
    Set up metrics endpoint and middleware for a FastAPI application.

    This function:
    1. Registers the service in metrics
    2. Adds a /metrics endpoint
    3. Adds middleware for automatic request timing

    Args:
        app: FastAPI application instance
        service_name: Name of the service
        port: Port the service runs on
        version: Service version

    Usage:
        from fastapi import FastAPI
        from shared.metrics import setup_metrics_endpoint

        app = FastAPI()
        setup_metrics_endpoint(app, "weather-rag", 8010)
    """
    from fastapi import Request, Response
    from starlette.middleware.base import BaseHTTPMiddleware

    # Register service
    register_service(service_name, version, port)

    # Add /metrics endpoint
    @app.get("/metrics", include_in_schema=False)
    async def metrics_endpoint():
        return Response(
            content=get_metrics_text(),
            media_type="text/plain; charset=utf-8"
        )

    # Add timing middleware
    class MetricsMiddleware(BaseHTTPMiddleware):
        async def dispatch(self, request: Request, call_next):
            # Skip metrics endpoint
            if request.url.path == "/metrics":
                return await call_next(request)

            method = request.method
            # Normalize path (remove query params, simplify dynamic segments)
            path = request.url.path

            # Track in-progress
            HTTP_REQUESTS_IN_PROGRESS.labels(
                service=service_name,
                method=method
            ).inc()

            start = time.time()
            status = "5xx"

            try:
                response = await call_next(request)
                status_code = response.status_code
                status = f"{status_code // 100}xx"
                return response
            except Exception as e:
                logger.error(
                    "request_error",
                    service=service_name,
                    method=method,
                    path=path,
                    error=str(e)
                )
                raise
            finally:
                duration = time.time() - start

                HTTP_REQUESTS_IN_PROGRESS.labels(
                    service=service_name,
                    method=method
                ).dec()

                record_http_request(service_name, method, path, status, duration)

    app.add_middleware(MetricsMiddleware)

    logger.info(
        "metrics_middleware_installed",
        service=service_name,
        port=port
    )
